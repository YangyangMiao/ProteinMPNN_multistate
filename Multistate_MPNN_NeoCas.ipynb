{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c64c22d-f1b5-46c2-86a8-ec80d05b2d70",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MultiStateMPNN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88ddaec0",
   "metadata": {},
   "source": [
    "# Takes multiple compatible structures with same lengths to generate a compatible sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6804765-3666-4c39-a010-30c54985c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings updated: 01/05/2023 14:00:15\n"
     ]
    }
   ],
   "source": [
    "### Design settings\n",
    "design_name = 'NeoCas_Trial3'\n",
    "pdb_path = '/work/lpdi/users/mpacesa/Projects/NeoCas/MultiStateMPNN/InputStates/'\n",
    "folder_for_outputs = '/work/lpdi/users/ymiao/ProteinMPNN/test_myy/output3'\n",
    "pdb_path_chains = 'A' #which chains to design, if none specified then all will be designed, space separated \"A B\"\n",
    "num_of_sequences = 50 #number of sequences to design\n",
    "sampling_temp = '1.0' #sampling temperature for amino acids, T=0.0 means taking argmax, T>>1.0 means sampling randomly.\")\n",
    "backbone_noise = 0.00 #backbone noise during sampling, 0.00-0.02 are good values\n",
    "omit_AAs = 'XC' #which amino acids to avoid, X is unkonwn, example 'XC' avoids unknown amino acids and cysteine\n",
    "positions_to_fix = '10 66 70 74 75 77 78 115 116 165 328 329 364 403 407 459 762 839 840 863 866 983 986 1122 1333 1335 1349' #which positions should remain fixed in the chains to be designed, separte positions by space and chains by comma (1 2 3, 21 24 25, ...), or enter \"none\" (PDB gets renumbered, starts from 1 always)\n",
    "invert_sel = None # If set to True, it will only design the positions selected above, basically inverting your selection\n",
    "\n",
    "### Pipeline settings\n",
    "# model settings\n",
    "main_path = '/work/lpdi/users/mpacesa/Pipelines/MultiStateMPNN' # path to your proteinMPNN installation\n",
    "model_name = 'v_48_020'\n",
    "solublempnn = False\n",
    "ca_only = False # Parse CA-only structures and use CA-only models\n",
    "num_seq_per_target = 1 # keep at 1 for multi-state\n",
    "batch_size = 1 # keep at 1 for multi-state\n",
    "seed_set = None # hard set the seed, set to number for testing purposes\n",
    "pssm_threshold = 0.00\n",
    "pssm_multi = 0.00\n",
    "\n",
    "# log settings\n",
    "save_score = True\n",
    "save_probs = True\n",
    "score_only = False\n",
    "conditional_probs_only = False\n",
    "unconditional_probs_only = False\n",
    "pssm_log_odds_flag = False\n",
    "pssm_bias_flag = False\n",
    "\n",
    "# custom JSON paths\n",
    "fixed_positions_jsonl = ''\n",
    "pssm_jsonl = ''\n",
    "omit_AA_jsonl = ''\n",
    "bias_AA_jsonl = ''\n",
    "tied_positions_jsonl = ''\n",
    "bias_by_res_jsonl = ''\n",
    "\n",
    "### Print time    \n",
    "from datetime import datetime\n",
    "time_date = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Settings updated: \"+time_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9f1f2-b83b-4ef2-a611-87acc005acc3",
   "metadata": {},
   "source": [
    "# Initialise functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6103fdcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:green'>Dependencies loaded</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Import dependencies\n",
    "import json, time, os, sys, glob\n",
    "import shutil\n",
    "import warnings\n",
    "import copy\n",
    "import csv\n",
    "import random\n",
    "import itertools\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown, display\n",
    "os.chdir(main_path)\n",
    "%matplotlib inline\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split, Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# proteinmpnn scripts\n",
    "from helper_scripts.parse_multiple_chains_multistate import ms_parse_chains\n",
    "from helper_scripts.assign_fixed_chains_multistate import ms_assign_chains\n",
    "from protein_mpnn_utils import loss_nll, loss_smoothed, gather_edges, gather_nodes, gather_nodes_t, cat_neighbors_nodes, _scores, _S_to_seq, tied_featurize, parse_PDB\n",
    "from protein_mpnn_utils import StructureDataset, StructureDatasetPDB, ProteinMPNN\n",
    "\n",
    "def printmd(string, color=None):\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))\n",
    "\n",
    "printmd(\"Dependencies loaded\", color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94eba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:green'>Paths have been created</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Build paths for experiment\n",
    "base_folder = folder_for_outputs\n",
    "if base_folder[-1] != '/':\n",
    "    base_folder = base_folder + '/'\n",
    "    \n",
    "if not os.path.exists(base_folder):\n",
    "    os.makedirs(base_folder)\n",
    "    \n",
    "if not os.path.exists(base_folder + 'seqs'):\n",
    "    os.makedirs(base_folder + 'seqs')\n",
    "    \n",
    "if save_score:\n",
    "    if not os.path.exists(base_folder + 'scores'):\n",
    "        os.makedirs(base_folder + 'scores')\n",
    "\n",
    "if score_only:\n",
    "    if not os.path.exists(base_folder + 'score_only'):\n",
    "        os.makedirs(base_folder + 'score_only')\n",
    "\n",
    "if conditional_probs_only:\n",
    "    if not os.path.exists(base_folder + 'conditional_probs_only'):\n",
    "        os.makedirs(base_folder + 'conditional_probs_only')\n",
    "\n",
    "if unconditional_probs_only:\n",
    "    if not os.path.exists(base_folder + 'unconditional_probs_only'):\n",
    "        os.makedirs(base_folder + 'unconditional_probs_only')\n",
    "\n",
    "if save_probs:\n",
    "    if not os.path.exists(base_folder + 'probs'):\n",
    "        os.makedirs(base_folder + 'probs') \n",
    "\n",
    "printmd(\"Paths have been created\", color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "199b58ba-02f4-43c4-97e9-4441fade162c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:green'>Accessory functions defined</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Define accessory functions\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def sample_sequence(master_pssm):\n",
    "    des_seq = []\n",
    "    for i in master_pssm[:92]:\n",
    "        sampled_aa = np.random.choice(np.arange(0, 21), p=i[0:21])\n",
    "        #print(alphabet[sampled_aa])\n",
    "        des_seq.append(alphabet[sampled_aa])\n",
    "        sampled_seq = ''.join(des_seq)\n",
    "        \n",
    "    return sampled_seq\n",
    "\n",
    "def max_sequence(master_pssm):\n",
    "    l = []\n",
    "    for res in np.argmax(master_pssm, axis=-1):\n",
    "        l.append(alphabet[res])\n",
    "    master_seq = ''.join(l)\n",
    "    \n",
    "    return master_seq\n",
    "\n",
    "def make_fixed_positions_dict(input_path, output_path, chain_list, position_list, invert_sel=None):\n",
    "    with open(input_path, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    \n",
    "    fixed_list = [[int(item) for item in one.split()] for one in position_list.split(\",\")]\n",
    "    global_designed_chain_list = [str(item) for item in chain_list.split()]\n",
    "    my_dict = {}\n",
    "    \n",
    "    if not invert_sel:\n",
    "        for json_str in json_list:\n",
    "            result = json.loads(json_str)\n",
    "            all_chain_list = [item[-1:] for item in list(result) if item[:9]=='seq_chain']\n",
    "            fixed_position_dict = {}\n",
    "            for i, chain in enumerate(global_designed_chain_list):\n",
    "                fixed_position_dict[chain] = fixed_list[i]\n",
    "            for chain in all_chain_list:\n",
    "                if chain not in global_designed_chain_list:       \n",
    "                    fixed_position_dict[chain] = []\n",
    "            my_dict[result['name']] = fixed_position_dict\n",
    "    else:\n",
    "        for json_str in json_list:\n",
    "            result = json.loads(json_str)\n",
    "            all_chain_list = [item[-1:] for item in list(result) if item[:9]=='seq_chain']\n",
    "            fixed_position_dict = {}   \n",
    "            for chain in all_chain_list:\n",
    "                seq_length = len(result[f'seq_chain_{chain}'])\n",
    "                all_residue_list = (np.arange(seq_length)+1).tolist()\n",
    "                if chain not in global_designed_chain_list:\n",
    "                    fixed_position_dict[chain] = all_residue_list\n",
    "                else:\n",
    "                    idx = np.argwhere(np.array(global_designed_chain_list) == chain)[0][0]\n",
    "                    fixed_position_dict[chain] = list(set(all_residue_list)-set(fixed_list[idx]))\n",
    "            my_dict[result['name']] = fixed_position_dict\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(json.dumps(my_dict) + '\\n')\n",
    "\n",
    "\n",
    "printmd(\"Accessory functions defined\", color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf37bdf-873e-46c8-977c-593c9c96f421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:green'>Paths and variable set</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Set important paths and variables\n",
    "# JSON paths\n",
    "jsonl_path = folder_for_outputs + '/parsed_pdbs.jsonl'\n",
    "chain_id_jsonl = folder_for_outputs + '/assigned_pdbs.jsonl'\n",
    "fixed_positions_jsonl = folder_for_outputs + '/fixed_positions.jsonl'\n",
    "\n",
    "# model paths\n",
    "if ca_only:\n",
    "    model_folder_path = main_path + '/ca_model_weights/'\n",
    "else:\n",
    "    if solublempnn:\n",
    "        model_folder_path = main_path + '/soluble_model_weights/'\n",
    "    else:\n",
    "        model_folder_path = main_path + '/vanilla_model_weights/'\n",
    "checkpoint_path = model_folder_path + f'{model_name}.pt'\n",
    "\n",
    "# AA alphabet\n",
    "omit_AAs_list = omit_AAs\n",
    "alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "chain_list = pdb_path_chains\n",
    "omit_AAs_np = np.array([AA in omit_AAs_list for AA in alphabet]).astype(np.float32)\n",
    "bias_AAs_np = np.zeros(len(alphabet))\n",
    "\n",
    "# NN settings\n",
    "max_length = 200000\n",
    "hidden_dim = 128\n",
    "num_layers = 3\n",
    "NUM_BATCHES = num_seq_per_target//batch_size\n",
    "BATCH_COPIES = batch_size\n",
    "temperatures = [float(item) for item in sampling_temp.split()]\n",
    "\n",
    "printmd(\"Paths and variable set\", color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53da63e6-94b6-4a4d-b82a-1f0f655cd0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "pssm_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "omit_AA_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "bias_AA_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "tied_positions_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "bias by residue dictionary is NOT loaded, or not provided\n",
      "----------------------------------------\n",
      "discarded {'bad_chars': 0, 'too_long': 0, 'bad_seq_length': 0}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Parse JSONs\n",
    "ms_parse_chains(pdb_path, jsonl_path, ca_only)\n",
    "ms_assign_chains(jsonl_path, chain_list, chain_id_jsonl)\n",
    "\n",
    "if os.path.isfile(chain_id_jsonl):\n",
    "    with open(chain_id_jsonl, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        chain_id_dict = json.loads(json_str)\n",
    "else:\n",
    "    chain_id_dict = None\n",
    "    print(40*'-')\n",
    "    print('chain_id_jsonl is NOT loaded')\n",
    "    \n",
    "if positions_to_fix != '':\n",
    "    make_fixed_positions_dict(input_path=jsonl_path, output_path=fixed_positions_jsonl, chain_list=chain_list, position_list=positions_to_fix, invert_sel=invert_sel)\n",
    "    \n",
    "if os.path.isfile(fixed_positions_jsonl):\n",
    "    with open(fixed_positions_jsonl, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        fixed_positions_dict = json.loads(json_str)\n",
    "else:\n",
    "    print(40*'-')\n",
    "    print('fixed_positions_jsonl is NOT loaded')\n",
    "    fixed_positions_dict = None\n",
    "    \n",
    "if os.path.isfile(pssm_jsonl):\n",
    "    with open(pssm_jsonl, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    pssm_dict = {}\n",
    "    for json_str in json_list:\n",
    "        pssm_dict.update(json.loads(json_str))\n",
    "else:\n",
    "    print(40*'-')\n",
    "    print('pssm_jsonl is NOT loaded')\n",
    "    pssm_dict = None\n",
    "    \n",
    "if os.path.isfile(omit_AA_jsonl):\n",
    "    with open(omit_AA_jsonl, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        omit_AA_dict = json.loads(json_str)\n",
    "else:\n",
    "    print(40*'-')\n",
    "    print('omit_AA_jsonl is NOT loaded')\n",
    "    omit_AA_dict = None\n",
    "\n",
    "if os.path.isfile(bias_AA_jsonl):\n",
    "    with open(bias_AA_jsonl, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        bias_AA_dict = json.loads(json_str)\n",
    "else:\n",
    "    print(40*'-')\n",
    "    print('bias_AA_jsonl is NOT loaded')\n",
    "    bias_AA_dict = None\n",
    "\n",
    "if bias_AA_dict:\n",
    "    for n, AA in enumerate(alphabet):\n",
    "        if AA in list(bias_AA_dict.keys()):\n",
    "            bias_AAs_np[n] = bias_AA_dict[AA]\n",
    "\n",
    "if os.path.isfile(tied_positions_jsonl):\n",
    "    with open(tied_positions_jsonl, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        tied_positions_dict = json.loads(json_str)\n",
    "else:\n",
    "    print(40*'-')\n",
    "    print('tied_positions_jsonl is NOT loaded')\n",
    "    tied_positions_dict = None\n",
    "    \n",
    "if os.path.isfile(bias_by_res_jsonl):\n",
    "    with open(bias_by_res_jsonl, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    \n",
    "    for json_str in json_list:\n",
    "        bias_by_res_dict = json.loads(json_str)\n",
    "    print('bias by residue dictionary is loaded')\n",
    "else:\n",
    "    print(40*'-')\n",
    "    print('bias by residue dictionary is NOT loaded, or not provided')\n",
    "    bias_by_res_dict = None\n",
    "print(40*'-')\n",
    "\n",
    "if pdb_path.endswith('.pdb'):\n",
    "    pdb_dict_list = parse_PDB(pdb_path, ca_only=ca_only)\n",
    "    dataset_valid = StructureDatasetPDB(pdb_dict_list, truncate=None, max_length=max_length)\n",
    "    all_chain_list = [item[-1:] for item in list(pdb_dict_list[0]) if item[:9]=='seq_chain'] #['A','B', 'C',...]\n",
    "    if pdb_path_chains:\n",
    "        designed_chain_list = [str(item) for item in pdb_path_chains.split()]\n",
    "    else:\n",
    "        designed_chain_list = all_chain_list\n",
    "    fixed_chain_list = [letter for letter in all_chain_list if letter not in designed_chain_list]\n",
    "    chain_id_dict = {}\n",
    "    chain_id_dict[pdb_dict_list[0]['name']]= (designed_chain_list, fixed_chain_list)\n",
    "else:\n",
    "    dataset_valid = StructureDataset(jsonl_path, truncate=None, max_length=max_length)    \n",
    "print(40*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab88709c-5efd-4dd3-b6c1-a7aab53d97d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating probabilities for: 1_binary_noMSA\n",
      "Generating probabilities for: 2_6bp_noMSA\n",
      "Generating probabilities for: 3_8bp_noMSA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# check device\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Model settings\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device) \n",
    "#print('Number of edges:', checkpoint['num_edges'])\n",
    "noise_level_print = checkpoint['noise_level']\n",
    "#print(f'Training noise level: {noise_level_print}A')\n",
    "\n",
    "# Load model\n",
    "model = ProteinMPNN(ca_only=ca_only, num_letters=21, node_features=hidden_dim, edge_features=hidden_dim, hidden_dim=hidden_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers, augment_eps=backbone_noise, k_neighbors=checkpoint['num_edges'])\n",
    "model.to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "model = ProteinMPNN(ca_only=ca_only, num_letters=21, node_features=hidden_dim, edge_features=hidden_dim, hidden_dim=hidden_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers, augment_eps=backbone_noise, k_neighbors=checkpoint['num_edges'])\n",
    "model.to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# reset Probabilities\n",
    "all_probs_list_list = []\n",
    "all_sample_list_list =[]\n",
    "chain_M_pos_list =[]\n",
    "# Timing\n",
    "start_time = time.time()\n",
    "total_residues = 0\n",
    "protein_list = []\n",
    "total_step = 0\n",
    "sample_encode_list =[]\n",
    "with torch.no_grad():\n",
    "    test_sum, test_weights = 0., 0.\n",
    "    #print('Generating sequences...')\n",
    "    for ix, protein in enumerate(dataset_valid):\n",
    "        score_list = []\n",
    "        global_score_list = []\n",
    "        all_probs_list = []\n",
    "        all_log_probs_list = []\n",
    "        S_sample_list = []\n",
    "        batch_clones = [copy.deepcopy(protein) for i in range(BATCH_COPIES)]\n",
    "        X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta = tied_featurize(batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict, pssm_dict, bias_by_res_dict, ca_only=ca_only)\n",
    "        pssm_log_odds_mask = (pssm_log_odds_all > pssm_threshold).float() #1.0 for true, 0.0 for false\n",
    "        name_ = batch_clones[0]['name']\n",
    "        if score_only:\n",
    "            structure_sequence_score_file = base_folder + '/score_only/' + batch_clones[0]['name'] + '.npz'\n",
    "            native_score_list = []\n",
    "            global_native_score_list = []\n",
    "            for j in range(NUM_BATCHES):\n",
    "                randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "                log_probs = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
    "                mask_for_loss = mask*chain_M*chain_M_pos\n",
    "                scores = _scores(S, log_probs, mask_for_loss)\n",
    "                native_score = scores.cpu().data.numpy()\n",
    "                native_score_list.append(native_score)\n",
    "                global_scores = _scores(S, log_probs, mask)\n",
    "                global_native_score = global_scores.cpu().data.numpy()\n",
    "                global_native_score_list.append(global_native_score)\n",
    "            native_score = np.concatenate(native_score_list, 0)\n",
    "            global_native_score = np.concatenate(global_native_score_list, 0)\n",
    "            ns_mean = native_score.mean()\n",
    "            ns_mean_print = np.format_float_positional(np.float32(ns_mean), unique=False, precision=4)\n",
    "            ns_std = native_score.std()\n",
    "            ns_std_print = np.format_float_positional(np.float32(ns_std), unique=False, precision=4)\n",
    "\n",
    "            global_ns_mean = global_native_score.mean()\n",
    "            global_ns_mean_print = np.format_float_positional(np.float32(global_ns_mean), unique=False, precision=4)\n",
    "            global_ns_std = global_native_score.std()\n",
    "            global_ns_std_print = np.format_float_positional(np.float32(global_ns_std), unique=False, precision=4)\n",
    "            ns_sample_size = native_score.shape[0]\n",
    "            np.savez(structure_sequence_score_file, score=native_score, global_score=global_native_score)\n",
    "            print(f'Score for {name_}, mean: {ns_mean_print}, std: {ns_std_print}, sample size: {ns_sample_size},  Global Score for {name_}, mean: {global_ns_mean_print}, std: {global_ns_std_print}, sample size: {ns_sample_size}')\n",
    "        elif conditional_probs_only:\n",
    "            print(f'Calculating conditional probabilities for {name_}')\n",
    "            conditional_probs_only_file = base_folder + '/conditional_probs_only/' + batch_clones[0]['name']\n",
    "            log_conditional_probs_list = []\n",
    "            for j in range(NUM_BATCHES):\n",
    "                randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "                log_conditional_probs = model.conditional_probs(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1, conditional_probs_only_backbone)\n",
    "                log_conditional_probs_list.append(log_conditional_probs.cpu().numpy())\n",
    "            concat_log_p = np.concatenate(log_conditional_probs_list, 0) #[B, L, 21]\n",
    "            mask_out = (chain_M*chain_M_pos*mask)[0,].cpu().numpy()\n",
    "            np.savez(conditional_probs_only_file, log_p=concat_log_p, S=S[0,].cpu().numpy(), mask=mask[0,].cpu().numpy(), design_mask=mask_out)\n",
    "        elif unconditional_probs_only:\n",
    "            print(f'Calculating sequence unconditional probabilities for {name_}')\n",
    "            unconditional_probs_only_file = base_folder + '/unconditional_probs_only/' + batch_clones[0]['name']\n",
    "            log_unconditional_probs_list = []\n",
    "            for j in range(NUM_BATCHES):\n",
    "                log_unconditional_probs = model.unconditional_probs(X, mask, residue_idx, chain_encoding_all)\n",
    "                log_unconditional_probs_list.append(log_unconditional_probs.cpu().numpy())\n",
    "            concat_log_p = np.concatenate(log_unconditional_probs_list, 0) #[B, L, 21]\n",
    "            mask_out = (chain_M*chain_M_pos*mask)[0,].cpu().numpy()\n",
    "            np.savez(unconditional_probs_only_file, log_p=concat_log_p, S=S[0,].cpu().numpy(), mask=mask[0,].cpu().numpy(), design_mask=mask_out)\n",
    "        else:\n",
    "            randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "            log_probs = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
    "            mask_for_loss = mask*chain_M*chain_M_pos\n",
    "            scores = _scores(S, log_probs, mask_for_loss) #score only the redesigned part\n",
    "            native_score = scores.cpu().data.numpy()\n",
    "            global_scores = _scores(S, log_probs, mask) #score the whole structure-sequence\n",
    "            global_native_score = global_scores.cpu().data.numpy()\n",
    "            # Generate some sequences\n",
    "            ali_file = base_folder + '/seqs/' + batch_clones[0]['name'] + '.fa'\n",
    "            score_file = base_folder + '/scores/' + batch_clones[0]['name'] + '.npz'\n",
    "            probs_file = base_folder + '/probs/' + batch_clones[0]['name'] + '.npz'\n",
    "            print(f'Generating probabilities for: {name_}')\n",
    "            t0 = time.time()\n",
    "            with open(ali_file, 'w') as f:\n",
    "                for temp in temperatures:\n",
    "                    for j in range(1):\n",
    "                        # print(j)\n",
    "                        randn_2 = torch.randn(chain_M.shape, device=X.device)\n",
    "                        if tied_positions_dict == None:\n",
    "                            sample_encode = model.sample_multistate_encode(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), bias_by_res=bias_by_res_all)\n",
    "                            # S_sample = sample_dict[\"S\"] \n",
    "                            sample_encode_list.append(sample_encode)\n",
    "                            # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35848733-dfa6-4547-8b04-29eff4947774",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Multi-state MPNN function\n",
    "\n",
    "np.random.seed(seed)\n",
    "def run_multistate_mpnn_test(dataset_valid, seed,state_weights):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # check device\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "    # Model settings\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device) \n",
    "    #print('Number of edges:', checkpoint['num_edges'])\n",
    "    noise_level_print = checkpoint['noise_level']\n",
    "    #print(f'Training noise level: {noise_level_print}A')\n",
    "\n",
    "    # Load model\n",
    "    model = ProteinMPNN(ca_only=ca_only, num_letters=21, node_features=hidden_dim, edge_features=hidden_dim, hidden_dim=hidden_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers, augment_eps=backbone_noise, k_neighbors=checkpoint['num_edges'])\n",
    "    model.to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # reset Probabilities\n",
    "    all_probs_list_list = []\n",
    "    all_sample_list_list =[]\n",
    "    chain_M_pos_list =[]\n",
    "    # Timing\n",
    "    start_time = time.time()\n",
    "    total_residues = 0\n",
    "    protein_list = []\n",
    "    total_step = 0\n",
    "    \n",
    "    sample_encode_list=[]\n",
    "    # Validation epoch\n",
    "    with torch.no_grad():\n",
    "        test_sum, test_weights = 0., 0.\n",
    "        #print('Generating sequences...')\n",
    "        for ix, protein in enumerate(dataset_valid):\n",
    "            score_list = []\n",
    "            global_score_list = []\n",
    "            all_probs_list = []\n",
    "            all_log_probs_list = []\n",
    "            S_sample_list = []\n",
    "            batch_clones = [copy.deepcopy(protein) for i in range(BATCH_COPIES)]\n",
    "            X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta = tied_featurize(batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict, pssm_dict, bias_by_res_dict, ca_only=ca_only)\n",
    "            pssm_log_odds_mask = (pssm_log_odds_all > pssm_threshold).float() #1.0 for true, 0.0 for false\n",
    "            name_ = batch_clones[0]['name']\n",
    "            # delet score only, conditional, unconditional prob mode\n",
    "            randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "            log_probs = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
    "            mask_for_loss = mask*chain_M*chain_M_pos\n",
    "            scores = _scores(S, log_probs, mask_for_loss) #score only the redesigned part\n",
    "            native_score = scores.cpu().data.numpy()\n",
    "            global_scores = _scores(S, log_probs, mask) #score the whole structure-sequence\n",
    "            global_native_score = global_scores.cpu().data.numpy()\n",
    "            # Generate some sequences\n",
    "            ali_file = base_folder + '/seqs/' + batch_clones[0]['name'] + '.fa'\n",
    "            score_file = base_folder + '/scores/' + batch_clones[0]['name'] + '.npz'\n",
    "            probs_file = base_folder + '/probs/' + batch_clones[0]['name'] + '.npz'\n",
    "            print(f'Generating probabilities for: {name_}')\n",
    "            t0 = time.time()\n",
    "            for temp in temperatures:\n",
    "                for j in range(NUM_BATCHES):\n",
    "                    if tied_positions_dict == None:\n",
    "                        sample_dict = model.sample(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), bias_by_res=bias_by_res_all)\n",
    "                        sample_encode = model.sample_multistate_encode(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), bias_by_res=bias_by_res_all)\n",
    "                        sample_encode_list.append(sample_encode)\n",
    "        output_dict_list=[]\n",
    "        decode_require_list=[]\n",
    "        myVars = vars()\n",
    "        for sample_encode in sample_encode_list:\n",
    "            namelist = ['N_nodes', 'decoding_order', 'chain_mask', 'mask', 'bias_by_res', 'S_true', 'E_idx', 'h_E', 'h_S','S','h_V', 'h_EXV_encoder_fw', 'h_V_stack', 'mask_bw', 'temperature', 'constant', 'constant_bias', 'pssm_bias_flag', 'pssm_coef', 'pssm_bias', 'pssm_log_odds_flag', 'pssm_multi', 'pssm_log_odds_mask', 'omit_AA_mask_flag', 'omit_AA_mask', 'all_probs']\n",
    "            for name in namelist:\n",
    "                myVars.__setitem__(name, sample_encode[name]) \n",
    "            decode_require =(decoding_order,chain_mask,mask,bias_by_res,S_true,E_idx,h_E,h_S,S,h_V,h_EXV_encoder_fw,h_V_stack,mask_bw,temperature,constant,constant_bias,pssm_bias_flag,pssm_coef,pssm_bias,pssm_log_odds_flag,pssm_multi,pssm_log_odds_mask,omit_AA_mask_flag,omit_AA_mask,all_probs)\n",
    "            decode_require_list.append(decode_require)\n",
    "        \n",
    "        sample_dict=model.sample_multistate_decode_N_nodes(N_nodes,decode_require_list,state_weights)\n",
    "        S_sample = sample_dict[\"S\"]\n",
    "        log_probs = model(X, S_sample, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_2, use_input_decoding_order=True, decoding_order=sample_dict[\"decoding_order\"])\n",
    "        mask_for_loss = mask*chain_M*chain_M_pos\n",
    "        scores = _scores(S_sample, log_probs, mask_for_loss)\n",
    "        scores = scores.cpu().data.numpy()\n",
    "\n",
    "        global_scores = _scores(S_sample, log_probs, mask) #score the whole structure-sequence\n",
    "        global_scores = global_scores.cpu().data.numpy()\n",
    "\n",
    "                \n",
    "    t1 = time.time()\n",
    "    dt = round(float(t1-start_time), 4)\n",
    "    num_seqs = len(temperatures)*NUM_BATCHES*BATCH_COPIES\n",
    "    total_length = X.shape[1]\n",
    "    print(f'{num_seqs} sequence of length {total_length} generated in {dt} seconds with score of {scores} ')\n",
    "    \n",
    "    \n",
    "    return sample_dict[\"probs\"].cpu().data.numpy(), S_sample.cpu().data.numpy(),chain_M_pos,scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b139e6ec-d2ae-447b-b895-4ab5598335f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>Seed value for design NeoCas_Trial3_0 is: 775</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating probabilities for: 1_binary_noMSA\n",
      "Generating probabilities for: 2_6bp_noMSA\n",
      "Generating probabilities for: 3_8bp_noMSA\n",
      "1 sequence of length 1368 generated in 31.6868 seconds with score of [1.8503491] \n",
      "REEPWSIGFDVGTDTVGVAVLGEKYKVPTRSFPVTGNSISHQRQTPLLGNLVIEPAQSREERIRKREKARIPKRRNNRMEELEKIFEPELKKKDFPFLLRIRLRDLPPHERSQSRHPLYGNDRLEVIYYKKFPTIQHRISTLANSSAKTQLIDIYVALKYLISNRGNFNLKGEIYPLFRDIQREANRLVLTFNHLFPQNPIDNSGVDFASILGADTSPEGRLAALQSTLPSRRPEDPFVRLVRLALGLQPDFAPSFQLANRALLNWEDEDYAERLERLLKEVGKRYRPLFEEALRLRDAILLGSWLTANDPAEISPLEEQEIMILNRHHKDRSHTKQLLKKEKPELVTRVFEDVPLNGYAGYEDGYATLEMFQEYIRPLLEDLPGTSFLLKQLGDRTLERELRSSTNKNIPTNVKHFTVDHILKKQQKYYPFIGEKKDLIVKIHEFVVPIDVGPLATDNTDNAVSRRKIDQPITSWNFTERIQYVMSRRKKTLNHTPRSPHNPSDEVLPKQNLTTMTYNVLNLLVRLRYLSQWLFAPRLLSATEIAALMMHLDLEKEEVTIDEIRTDYFGLILQYADVPIEGEKERLHAKLHMKLNLNDIIKPPLVLLDDSNVALIDEIVDTLVRYSTAIIIAEELRKWMNYLTKEQMITLVNMKLDGWSWKSKNEIDGFRDQTTQKSDMDYLADGGERQLNILEIYSDPDLSFGGIVAHIKIRAHGRNLEASIDYIPWLPAMSKVGLLMFRIYMENIDTHPSRMTRNIFMEIETVRTNGSKHTAEPEKILRGVKEGLARLGSDLMREMPVEPWQLLDLLWRLWYQSNGFDFYVLSSLEYSRLQLRSVDHVISRAFLDTEATHNLVLVSSRQNQGKNHRAVTAQISQRRFSYWQRLKEAFLITQAQFNTLTMVQTAGYSAEEIWEDVIDEIKKQDSRQATVAEIIANFANTEKDEDEKMIHTTQVLTLTDTQVEKFYRDFELNQVIKIDNRYHAQDAYMVGVLGQKIIELYPQLLAYLVTGSAKWKDPETWVVSEGEQNVFTTPNVLYDGHMLFFLSTERKPRLSIDRKKPRIIHNSRTFVTVFNRKIDFNILLNNFNLKDIHDVVLEERLKGNDFTQKWASQTQSTNLMRRKPNHDPFLYGGYEQPREQHVVYVLARKYSGYRKRLNRVQELIGLTIKQEEAYQANANIGLIQRGYLEVQMNDILELPPHDLFVLDRERKFLLADATELARGNNKQVKYHVISWLSRASVERAQVGEKSAIRKENDFVVRNDHLFATLMGDLSRWMWQFEYDERRLNKLQRAYYQNRNWPIQELARHMIGVFTVWRVGMARDYKFGDNLLDRARQTPTTQLLSADLVHENLTMAEVHVLRLTEMQLA [1.8503491]\n"
     ]
    }
   ],
   "source": [
    "seq_list=[]\n",
    "dataset_valid = dataset_valid[0:3]\n",
    "state_weights = np.array([1.0, 1.0, 1.0])\n",
    "state_weights = torch.from_numpy(state_weights/np.sum(state_weights))\n",
    "\n",
    "for i in range(0,1):\n",
    "    if seed_set is None:\n",
    "        seed = int(np.random.randint(0, high=9999, size=1, dtype=int)[0])\n",
    "    else:\n",
    "        seed = seed_set\n",
    "\n",
    "    sequence_name = design_name+'_'+str(i)\n",
    "    \n",
    "    printmd(\"Seed value for design \"+sequence_name+\" is: \"+str(seed), color=\"blue\")\n",
    "    prob,sample,chain_M_pos, scores= run_multistate_mpnn_test(dataset_valid, seed,state_weights)\n",
    "    l = []\n",
    "    for res in sample[0]:\n",
    "        l.append(alphabet[int(res)])\n",
    "    master_seq = ''.join(l)    \n",
    "    seq_list.append(master_seq)\n",
    "    print(master_seq,scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e6e2205-6ef5-4360-9a11-6a6ff96b607a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPELFSINFDVGNTSIGYAVLDDSFAIPSLTFEVTGNTNKTKHERPLIGVLLAPAAKDHVRYYYERRARRRRARRQNRQTLLAAYFAAIMDQEDPYFMERVRQMYLPYKDRNTHRHARYGNKEKEREHYEAYPNIWLLIQQLVNKPTKAPLRDGYTALSFLLSNRGNNEMKGIVNVLKADVQEEFLNSVSLYNKRFPSAPIDTTGIDASGILNAWTSADDRLAALMGQLPHIAADSYFGHLVALVPGLVPNQISAFGLREEALLDFVDPGFAHRLKQNLELVILENRELFDSALDLGWAIKLGSTLTSRDANSRSPESKQKVDDQMKHHEDEEKFYDLVTKQNPGEKEEIAENKESNGSAGFRDGFANWEIYKNKILPILQEMEGTDELISKIEKNEFLRPLRTDENRYMDRDLYLHDLGEILKQQQQFFPFLREKEEHIIQLARYRVPAEVGPLLKKNSEDEDRAFKIQLAISPSNYYGVVDWITSAVRKFHLLTPTSPNLPTFRVLALQSLRNQTFRTFNELATITYLSKQLAEPKKLSSSDALDLYETLFLINAYVTVNMVNTMFFKTIKKWANVTLSGHQNQFPTKLGMYHELLEITKNEPLLHDGSKWARIDQIIDILTRAKNKVFIANGKSQFAHDFTAAEMDKITQFVTEGWGDFSVQLITGLLDDVTGKSVMDFLKDDGTERRTFDQIINDPTLSQKRQIQWLSSRAQDWSLPYLVANLAAPPSKKKGISYTLNILGEIIELTGGENPAVVAIEFSVIRSSINTDLLTASQYLTQINRGAQTLATSILQDYPVKTVDLFSIKLYLWFRQQMMDAWNPETLDPRMLEQYDIDHIKSVRYRETNDIGNLALVKSRSNRGKNAAHPSEKVTARLNQDHTALLAAELIDARTYKALTMSYSAGLWLKTIYDYILESIELKEMISTRLAELIASMLNDQQDGNGKPVYKVKVIRLNSALIDIFRQNHKLYSVNRISPKIHAFDAYMTGLLGSRLISRYPEAWAWLTNGEYQETDFATFIGPPGAIDVAALPFLFSNGAILGWQLKTALTADGNKVNYPLKITHPSTKELFFDPETNLSVIKKTFNLPKVNVVVWTEVRTGETNDRILNNAAQSDTLMPREPFHNAKIYGSYWNPVISESLLVLATWVKGKSRILSPRRTIIGITILELKKFNTDPLEFLQTKGFKDINTIYLLRLHPHALFQLQVGQNWLLLSEKELATGNYLTADFEMVAGLFLASRYYPRVGDKDYRAEDQQYMLEREEYLDDLVKLLPEYSTQYKNASDRHLKFVSTFESWKELPLEERAEELLHAFQFIRLGESRAFLLHEVLNERSRRTDTELLWSGTLIHLDLVGFRRNDLSYASLGNS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(master_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6ce8436-7ecb-4315-9b8f-37422fa0e4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:green'>MPNN Function defined</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Define Multi-state MPNN function\n",
    "\n",
    "def run_multistate_mpnn_test(dataset_valid, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # check device\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "    # Model settings\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device) \n",
    "    #print('Number of edges:', checkpoint['num_edges'])\n",
    "    noise_level_print = checkpoint['noise_level']\n",
    "    #print(f'Training noise level: {noise_level_print}A')\n",
    "\n",
    "    # Load model\n",
    "    model = ProteinMPNN(ca_only=ca_only, num_letters=21, node_features=hidden_dim, edge_features=hidden_dim, hidden_dim=hidden_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers, augment_eps=backbone_noise, k_neighbors=checkpoint['num_edges'])\n",
    "    model.to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # reset Probabilities\n",
    "    all_probs_list_list = []\n",
    "    all_sample_list_list =[]\n",
    "    chain_M_pos_list =[]\n",
    "    # Timing\n",
    "    start_time = time.time()\n",
    "    total_residues = 0\n",
    "    protein_list = []\n",
    "    total_step = 0\n",
    "\n",
    "    # Validation epoch\n",
    "    with torch.no_grad():\n",
    "        test_sum, test_weights = 0., 0.\n",
    "        #print('Generating sequences...')\n",
    "        for ix, protein in enumerate(dataset_valid):\n",
    "            score_list = []\n",
    "            global_score_list = []\n",
    "            all_probs_list = []\n",
    "            all_log_probs_list = []\n",
    "            S_sample_list = []\n",
    "            batch_clones = [copy.deepcopy(protein) for i in range(BATCH_COPIES)]\n",
    "            X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta = tied_featurize(batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict, pssm_dict, bias_by_res_dict, ca_only=ca_only)\n",
    "            pssm_log_odds_mask = (pssm_log_odds_all > pssm_threshold).float() #1.0 for true, 0.0 for false\n",
    "            name_ = batch_clones[0]['name']\n",
    "            if score_only:\n",
    "                structure_sequence_score_file = base_folder + '/score_only/' + batch_clones[0]['name'] + '.npz'\n",
    "                native_score_list = []\n",
    "                global_native_score_list = []\n",
    "                for j in range(NUM_BATCHES):\n",
    "                    randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "                    log_probs = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
    "                    mask_for_loss = mask*chain_M*chain_M_pos\n",
    "                    scores = _scores(S, log_probs, mask_for_loss)\n",
    "                    native_score = scores.cpu().data.numpy()\n",
    "                    native_score_list.append(native_score)\n",
    "                    global_scores = _scores(S, log_probs, mask)\n",
    "                    global_native_score = global_scores.cpu().data.numpy()\n",
    "                    global_native_score_list.append(global_native_score)\n",
    "                native_score = np.concatenate(native_score_list, 0)\n",
    "                global_native_score = np.concatenate(global_native_score_list, 0)\n",
    "                ns_mean = native_score.mean()\n",
    "                ns_mean_print = np.format_float_positional(np.float32(ns_mean), unique=False, precision=4)\n",
    "                ns_std = native_score.std()\n",
    "                ns_std_print = np.format_float_positional(np.float32(ns_std), unique=False, precision=4)\n",
    "\n",
    "                global_ns_mean = global_native_score.mean()\n",
    "                global_ns_mean_print = np.format_float_positional(np.float32(global_ns_mean), unique=False, precision=4)\n",
    "                global_ns_std = global_native_score.std()\n",
    "                global_ns_std_print = np.format_float_positional(np.float32(global_ns_std), unique=False, precision=4)\n",
    "                ns_sample_size = native_score.shape[0]\n",
    "                np.savez(structure_sequence_score_file, score=native_score, global_score=global_native_score)\n",
    "                print(f'Score for {name_}, mean: {ns_mean_print}, std: {ns_std_print}, sample size: {ns_sample_size},  Global Score for {name_}, mean: {global_ns_mean_print}, std: {global_ns_std_print}, sample size: {ns_sample_size}')\n",
    "            elif conditional_probs_only:\n",
    "                print(f'Calculating conditional probabilities for {name_}')\n",
    "                conditional_probs_only_file = base_folder + '/conditional_probs_only/' + batch_clones[0]['name']\n",
    "                log_conditional_probs_list = []\n",
    "                for j in range(NUM_BATCHES):\n",
    "                    randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "                    log_conditional_probs = model.conditional_probs(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1, conditional_probs_only_backbone)\n",
    "                    log_conditional_probs_list.append(log_conditional_probs.cpu().numpy())\n",
    "                concat_log_p = np.concatenate(log_conditional_probs_list, 0) #[B, L, 21]\n",
    "                mask_out = (chain_M*chain_M_pos*mask)[0,].cpu().numpy()\n",
    "                np.savez(conditional_probs_only_file, log_p=concat_log_p, S=S[0,].cpu().numpy(), mask=mask[0,].cpu().numpy(), design_mask=mask_out)\n",
    "            elif unconditional_probs_only:\n",
    "                print(f'Calculating sequence unconditional probabilities for {name_}')\n",
    "                unconditional_probs_only_file = base_folder + '/unconditional_probs_only/' + batch_clones[0]['name']\n",
    "                log_unconditional_probs_list = []\n",
    "                for j in range(NUM_BATCHES):\n",
    "                    log_unconditional_probs = model.unconditional_probs(X, mask, residue_idx, chain_encoding_all)\n",
    "                    log_unconditional_probs_list.append(log_unconditional_probs.cpu().numpy())\n",
    "                concat_log_p = np.concatenate(log_unconditional_probs_list, 0) #[B, L, 21]\n",
    "                mask_out = (chain_M*chain_M_pos*mask)[0,].cpu().numpy()\n",
    "                np.savez(unconditional_probs_only_file, log_p=concat_log_p, S=S[0,].cpu().numpy(), mask=mask[0,].cpu().numpy(), design_mask=mask_out)\n",
    "            else:\n",
    "                randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "                log_probs = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
    "                mask_for_loss = mask*chain_M*chain_M_pos\n",
    "                scores = _scores(S, log_probs, mask_for_loss) #score only the redesigned part\n",
    "                native_score = scores.cpu().data.numpy()\n",
    "                global_scores = _scores(S, log_probs, mask) #score the whole structure-sequence\n",
    "                global_native_score = global_scores.cpu().data.numpy()\n",
    "                # Generate some sequences\n",
    "                ali_file = base_folder + '/seqs/' + batch_clones[0]['name'] + '.fa'\n",
    "                score_file = base_folder + '/scores/' + batch_clones[0]['name'] + '.npz'\n",
    "                probs_file = base_folder + '/probs/' + batch_clones[0]['name'] + '.npz'\n",
    "                print(f'Generating probabilities for: {name_}')\n",
    "                t0 = time.time()\n",
    "                with open(ali_file, 'w') as f:\n",
    "                    for temp in temperatures:\n",
    "                        for j in range(NUM_BATCHES):\n",
    "                            # print(j)\n",
    "                            randn_2 = torch.randn(chain_M.shape, device=X.device)\n",
    "                            if tied_positions_dict == None:\n",
    "                                sample_dict = model.sample(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), bias_by_res=bias_by_res_all)\n",
    "                                S_sample = sample_dict[\"S\"] \n",
    "                            else:\n",
    "                                sample_dict = model.tied_sample(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), tied_pos=tied_pos_list_of_lists_list[0], tied_beta=tied_beta, bias_by_res=bias_by_res_all)\n",
    "                                # Compute scores\n",
    "                                S_sample = sample_dict[\"S\"]\n",
    "                            log_probs = model(X, S_sample, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_2, use_input_decoding_order=True, decoding_order=sample_dict[\"decoding_order\"])\n",
    "                            mask_for_loss = mask*chain_M*chain_M_pos\n",
    "                            scores = _scores(S_sample, log_probs, mask_for_loss)\n",
    "                            scores = scores.cpu().data.numpy()\n",
    "\n",
    "                            global_scores = _scores(S_sample, log_probs, mask) #score the whole structure-sequence\n",
    "                            global_scores = global_scores.cpu().data.numpy()\n",
    "\n",
    "                            all_probs_list.append(sample_dict[\"probs\"].cpu().data.numpy())\n",
    "                            all_log_probs_list.append(log_probs.cpu().data.numpy())\n",
    "                            S_sample_list.append(S_sample.cpu().data.numpy())\n",
    "                            for b_ix in range(BATCH_COPIES):\n",
    "                                masked_chain_length_list = masked_chain_length_list_list[b_ix]\n",
    "                                masked_list = masked_list_list[b_ix]\n",
    "                                seq_recovery_rate = torch.sum(torch.sum(torch.nn.functional.one_hot(S[b_ix], 21)*torch.nn.functional.one_hot(S_sample[b_ix], 21),axis=-1)*mask_for_loss[b_ix])/torch.sum(mask_for_loss[b_ix])\n",
    "                                seq = _S_to_seq(S_sample[b_ix], chain_M[b_ix])\n",
    "                                score = scores[b_ix]\n",
    "                                score_list.append(score)\n",
    "                                global_score = global_scores[b_ix]\n",
    "                                global_score_list.append(global_score)\n",
    "                                native_seq = _S_to_seq(S[b_ix], chain_M[b_ix])\n",
    "                                if b_ix == 0 and j==0 and temp==temperatures[0]:\n",
    "                                    start = 0\n",
    "                                    end = 0\n",
    "                                    list_of_AAs = []\n",
    "                                    for mask_l in masked_chain_length_list:\n",
    "                                        end += mask_l\n",
    "                                        list_of_AAs.append(native_seq[start:end])\n",
    "                                        start = end\n",
    "                                    native_seq = \"\".join(list(np.array(list_of_AAs)[np.argsort(masked_list)]))\n",
    "                                    l0 = 0\n",
    "                                    for mc_length in list(np.array(masked_chain_length_list)[np.argsort(masked_list)])[:-1]:\n",
    "                                        l0 += mc_length\n",
    "                                        native_seq = native_seq[:l0] + '/' + native_seq[l0:]\n",
    "                                        l0 += 1\n",
    "\n",
    "                                    sorted_masked_chain_letters = np.argsort(masked_list_list[0])\n",
    "                                    print_masked_chains = [masked_list_list[0][i] for i in sorted_masked_chain_letters]\n",
    "                                    sorted_visible_chain_letters = np.argsort(visible_list_list[0])\n",
    "                                    print_visible_chains = [visible_list_list[0][i] for i in sorted_visible_chain_letters]\n",
    "                                    native_score_print = np.format_float_positional(np.float32(native_score.mean()), unique=False, precision=4)\n",
    "                                    global_native_score_print = np.format_float_positional(np.float32(global_native_score.mean()), unique=False, precision=4)\n",
    "                                    # script_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "                                    if ca_only:\n",
    "                                        print_model_name = 'CA_model_name'\n",
    "                                    else:\n",
    "                                        print_model_name = 'model_name'\n",
    "\n",
    "                                    f.write('>{}, score={}, global_score={}, fixed_chains={}, designed_chains={}, {}={}, seed={}\\n{}\\n'.format(name_, native_score_print, global_native_score_print, print_visible_chains, print_masked_chains, print_model_name, model_name, seed, native_seq)) #write the native sequence\n",
    "                                start = 0\n",
    "                                end = 0\n",
    "                                list_of_AAs = []\n",
    "                                for mask_l in masked_chain_length_list:\n",
    "                                    end += mask_l\n",
    "                                    list_of_AAs.append(seq[start:end])\n",
    "                                    start = end\n",
    "\n",
    "                                seq = \"\".join(list(np.array(list_of_AAs)[np.argsort(masked_list)]))\n",
    "                                l0 = 0\n",
    "                                for mc_length in list(np.array(masked_chain_length_list)[np.argsort(masked_list)])[:-1]:\n",
    "                                    l0 += mc_length\n",
    "                                    seq = seq[:l0] + '/' + seq[l0:]\n",
    "                                    l0 += 1\n",
    "                                score_print = np.format_float_positional(np.float32(score), unique=False, precision=4)\n",
    "                                global_score_print = np.format_float_positional(np.float32(global_score), unique=False, precision=4)\n",
    "                                seq_rec_print = np.format_float_positional(np.float32(seq_recovery_rate.detach().cpu().numpy()), unique=False, precision=4)\n",
    "                                sample_number = j*BATCH_COPIES+b_ix+1\n",
    "                                f.write('>T={}, sample={}, score={}, global_score={}, seq_recovery={}\\n{}\\n'.format(temp,sample_number, score_print, global_score_print, seq_rec_print, seq)) #write generated sequence\n",
    "                if save_score:\n",
    "                    np.savez(score_file, score=np.array(score_list, np.float32), global_score=np.array(global_score_list, np.float32))\n",
    "                if save_probs:\n",
    "                    all_probs_concat = np.concatenate(all_probs_list)\n",
    "                    all_log_probs_concat = np.concatenate(all_log_probs_list)\n",
    "                    S_sample_concat = np.concatenate(S_sample_list)\n",
    "                    np.savez(probs_file, probs=np.array(all_probs_concat, np.float32), log_probs=np.array(all_log_probs_concat, np.float32), S=np.array(S_sample_concat, np.int32), mask=mask_for_loss.cpu().data.numpy(), chain_order=chain_list_list)\n",
    "                \n",
    "                all_probs_list_list.append(all_probs_list)\n",
    "                all_sample_list_list.append(S_sample_list)\n",
    "                chain_M_pos_list.append(chain_M_pos)                \n",
    "    t1 = time.time()\n",
    "    dt = round(float(t1-start_time), 4)\n",
    "    num_seqs = len(temperatures)*NUM_BATCHES*BATCH_COPIES\n",
    "    total_length = X.shape[1]\n",
    "    print(f'{num_seqs} sequence of length {total_length} generated in {dt} seconds with score of {score_print} and sequence recovery of {seq_rec_print}')\n",
    "    \n",
    "    \n",
    "    return all_probs_list_list, all_sample_list_list,chain_M_pos_list,score_print, seq_rec_print\n",
    "\n",
    "printmd(\"MPNN Function defined\", color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35f4e195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:green'>MPNN Function defined</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Define Multi-state MPNN function\n",
    "def run_multistate_mpnn(dataset_valid, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # check device\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "    # Model settings\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device) \n",
    "    #print('Number of edges:', checkpoint['num_edges'])\n",
    "    noise_level_print = checkpoint['noise_level']\n",
    "    #print(f'Training noise level: {noise_level_print}A')\n",
    "\n",
    "    # Load model\n",
    "    model = ProteinMPNN(ca_only=ca_only, num_letters=21, node_features=hidden_dim, edge_features=hidden_dim, hidden_dim=hidden_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers, augment_eps=backbone_noise, k_neighbors=checkpoint['num_edges'])\n",
    "    model.to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # reset Probabilities\n",
    "    all_probs_list_list = []\n",
    "    all_sample_list_list =[]\n",
    "    chain_M_pos_list =[]\n",
    "    # Timing\n",
    "    start_time = time.time()\n",
    "    total_residues = 0\n",
    "    protein_list = []\n",
    "    total_step = 0\n",
    "\n",
    "    # Validation epoch\n",
    "    with torch.no_grad():\n",
    "        test_sum, test_weights = 0., 0.\n",
    "        #print('Generating sequences...')\n",
    "        for ix, protein in enumerate(dataset_valid):\n",
    "            score_list = []\n",
    "            global_score_list = []\n",
    "            all_probs_list = []\n",
    "            all_log_probs_list = []\n",
    "            S_sample_list = []\n",
    "            batch_clones = [copy.deepcopy(protein) for i in range(BATCH_COPIES)]\n",
    "            X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta = tied_featurize(batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict, pssm_dict, bias_by_res_dict, ca_only=ca_only)\n",
    "            pssm_log_odds_mask = (pssm_log_odds_all > pssm_threshold).float() #1.0 for true, 0.0 for false\n",
    "            name_ = batch_clones[0]['name']\n",
    "            if score_only:\n",
    "                structure_sequence_score_file = base_folder + '/score_only/' + batch_clones[0]['name'] + '.npz'\n",
    "                native_score_list = []\n",
    "                global_native_score_list = []\n",
    "                for j in range(NUM_BATCHES):\n",
    "                    randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "                    log_probs = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
    "                    mask_for_loss = mask*chain_M*chain_M_pos\n",
    "                    scores = _scores(S, log_probs, mask_for_loss)\n",
    "                    native_score = scores.cpu().data.numpy()\n",
    "                    native_score_list.append(native_score)\n",
    "                    global_scores = _scores(S, log_probs, mask)\n",
    "                    global_native_score = global_scores.cpu().data.numpy()\n",
    "                    global_native_score_list.append(global_native_score)\n",
    "                native_score = np.concatenate(native_score_list, 0)\n",
    "                global_native_score = np.concatenate(global_native_score_list, 0)\n",
    "                ns_mean = native_score.mean()\n",
    "                ns_mean_print = np.format_float_positional(np.float32(ns_mean), unique=False, precision=4)\n",
    "                ns_std = native_score.std()\n",
    "                ns_std_print = np.format_float_positional(np.float32(ns_std), unique=False, precision=4)\n",
    "\n",
    "                global_ns_mean = global_native_score.mean()\n",
    "                global_ns_mean_print = np.format_float_positional(np.float32(global_ns_mean), unique=False, precision=4)\n",
    "                global_ns_std = global_native_score.std()\n",
    "                global_ns_std_print = np.format_float_positional(np.float32(global_ns_std), unique=False, precision=4)\n",
    "                ns_sample_size = native_score.shape[0]\n",
    "                np.savez(structure_sequence_score_file, score=native_score, global_score=global_native_score)\n",
    "                print(f'Score for {name_}, mean: {ns_mean_print}, std: {ns_std_print}, sample size: {ns_sample_size},  Global Score for {name_}, mean: {global_ns_mean_print}, std: {global_ns_std_print}, sample size: {ns_sample_size}')\n",
    "            elif conditional_probs_only:\n",
    "                print(f'Calculating conditional probabilities for {name_}')\n",
    "                conditional_probs_only_file = base_folder + '/conditional_probs_only/' + batch_clones[0]['name']\n",
    "                log_conditional_probs_list = []\n",
    "                for j in range(NUM_BATCHES):\n",
    "                    randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "                    log_conditional_probs = model.conditional_probs(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1, conditional_probs_only_backbone)\n",
    "                    log_conditional_probs_list.append(log_conditional_probs.cpu().numpy())\n",
    "                concat_log_p = np.concatenate(log_conditional_probs_list, 0) #[B, L, 21]\n",
    "                mask_out = (chain_M*chain_M_pos*mask)[0,].cpu().numpy()\n",
    "                np.savez(conditional_probs_only_file, log_p=concat_log_p, S=S[0,].cpu().numpy(), mask=mask[0,].cpu().numpy(), design_mask=mask_out)\n",
    "            elif unconditional_probs_only:\n",
    "                print(f'Calculating sequence unconditional probabilities for {name_}')\n",
    "                unconditional_probs_only_file = base_folder + '/unconditional_probs_only/' + batch_clones[0]['name']\n",
    "                log_unconditional_probs_list = []\n",
    "                for j in range(NUM_BATCHES):\n",
    "                    log_unconditional_probs = model.unconditional_probs(X, mask, residue_idx, chain_encoding_all)\n",
    "                    log_unconditional_probs_list.append(log_unconditional_probs.cpu().numpy())\n",
    "                concat_log_p = np.concatenate(log_unconditional_probs_list, 0) #[B, L, 21]\n",
    "                mask_out = (chain_M*chain_M_pos*mask)[0,].cpu().numpy()\n",
    "                np.savez(unconditional_probs_only_file, log_p=concat_log_p, S=S[0,].cpu().numpy(), mask=mask[0,].cpu().numpy(), design_mask=mask_out)\n",
    "            else:\n",
    "                randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "                log_probs = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
    "                mask_for_loss = mask*chain_M*chain_M_pos\n",
    "                scores = _scores(S, log_probs, mask_for_loss) #score only the redesigned part\n",
    "                native_score = scores.cpu().data.numpy()\n",
    "                global_scores = _scores(S, log_probs, mask) #score the whole structure-sequence\n",
    "                global_native_score = global_scores.cpu().data.numpy()\n",
    "                # Generate some sequences\n",
    "                ali_file = base_folder + '/seqs/' + batch_clones[0]['name'] + '.fa'\n",
    "                score_file = base_folder + '/scores/' + batch_clones[0]['name'] + '.npz'\n",
    "                probs_file = base_folder + '/probs/' + batch_clones[0]['name'] + '.npz'\n",
    "                print(f'Generating probabilities for: {name_}')\n",
    "                t0 = time.time()\n",
    "                with open(ali_file, 'w') as f:\n",
    "                    for temp in temperatures:\n",
    "                        for j in range(NUM_BATCHES):\n",
    "                            # print(j)\n",
    "                            randn_2 = torch.randn(chain_M.shape, device=X.device)\n",
    "                            if tied_positions_dict == None:\n",
    "                                sample_dict = model.sample(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), bias_by_res=bias_by_res_all)\n",
    "                                S_sample = sample_dict[\"S\"] \n",
    "                            else:\n",
    "                                sample_dict = model.tied_sample(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), tied_pos=tied_pos_list_of_lists_list[0], tied_beta=tied_beta, bias_by_res=bias_by_res_all)\n",
    "                                # Compute scores\n",
    "                                S_sample = sample_dict[\"S\"]\n",
    "                            log_probs = model(X, S_sample, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_2, use_input_decoding_order=True, decoding_order=sample_dict[\"decoding_order\"])\n",
    "                            mask_for_loss = mask*chain_M*chain_M_pos\n",
    "                            scores = _scores(S_sample, log_probs, mask_for_loss)\n",
    "                            scores = scores.cpu().data.numpy()\n",
    "\n",
    "                            global_scores = _scores(S_sample, log_probs, mask) #score the whole structure-sequence\n",
    "                            global_scores = global_scores.cpu().data.numpy()\n",
    "\n",
    "                            all_probs_list.append(sample_dict[\"probs\"].cpu().data.numpy())\n",
    "                            all_log_probs_list.append(log_probs.cpu().data.numpy())\n",
    "                            S_sample_list.append(S_sample.cpu().data.numpy())\n",
    "                            for b_ix in range(BATCH_COPIES):\n",
    "                                masked_chain_length_list = masked_chain_length_list_list[b_ix]\n",
    "                                masked_list = masked_list_list[b_ix]\n",
    "                                seq_recovery_rate = torch.sum(torch.sum(torch.nn.functional.one_hot(S[b_ix], 21)*torch.nn.functional.one_hot(S_sample[b_ix], 21),axis=-1)*mask_for_loss[b_ix])/torch.sum(mask_for_loss[b_ix])\n",
    "                                seq = _S_to_seq(S_sample[b_ix], chain_M[b_ix])\n",
    "                                score = scores[b_ix]\n",
    "                                score_list.append(score)\n",
    "                                global_score = global_scores[b_ix]\n",
    "                                global_score_list.append(global_score)\n",
    "                                native_seq = _S_to_seq(S[b_ix], chain_M[b_ix])\n",
    "                                if b_ix == 0 and j==0 and temp==temperatures[0]:\n",
    "                                    start = 0\n",
    "                                    end = 0\n",
    "                                    list_of_AAs = []\n",
    "                                    for mask_l in masked_chain_length_list:\n",
    "                                        end += mask_l\n",
    "                                        list_of_AAs.append(native_seq[start:end])\n",
    "                                        start = end\n",
    "                                    native_seq = \"\".join(list(np.array(list_of_AAs)[np.argsort(masked_list)]))\n",
    "                                    l0 = 0\n",
    "                                    for mc_length in list(np.array(masked_chain_length_list)[np.argsort(masked_list)])[:-1]:\n",
    "                                        l0 += mc_length\n",
    "                                        native_seq = native_seq[:l0] + '/' + native_seq[l0:]\n",
    "                                        l0 += 1\n",
    "\n",
    "                                    sorted_masked_chain_letters = np.argsort(masked_list_list[0])\n",
    "                                    print_masked_chains = [masked_list_list[0][i] for i in sorted_masked_chain_letters]\n",
    "                                    sorted_visible_chain_letters = np.argsort(visible_list_list[0])\n",
    "                                    print_visible_chains = [visible_list_list[0][i] for i in sorted_visible_chain_letters]\n",
    "                                    native_score_print = np.format_float_positional(np.float32(native_score.mean()), unique=False, precision=4)\n",
    "                                    global_native_score_print = np.format_float_positional(np.float32(global_native_score.mean()), unique=False, precision=4)\n",
    "                                    # script_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "                                    if ca_only:\n",
    "                                        print_model_name = 'CA_model_name'\n",
    "                                    else:\n",
    "                                        print_model_name = 'model_name'\n",
    "\n",
    "                                    f.write('>{}, score={}, global_score={}, fixed_chains={}, designed_chains={}, {}={}, seed={}\\n{}\\n'.format(name_, native_score_print, global_native_score_print, print_visible_chains, print_masked_chains, print_model_name, model_name, seed, native_seq)) #write the native sequence\n",
    "                                start = 0\n",
    "                                end = 0\n",
    "                                list_of_AAs = []\n",
    "                                for mask_l in masked_chain_length_list:\n",
    "                                    end += mask_l\n",
    "                                    list_of_AAs.append(seq[start:end])\n",
    "                                    start = end\n",
    "\n",
    "                                seq = \"\".join(list(np.array(list_of_AAs)[np.argsort(masked_list)]))\n",
    "                                l0 = 0\n",
    "                                for mc_length in list(np.array(masked_chain_length_list)[np.argsort(masked_list)])[:-1]:\n",
    "                                    l0 += mc_length\n",
    "                                    seq = seq[:l0] + '/' + seq[l0:]\n",
    "                                    l0 += 1\n",
    "                                score_print = np.format_float_positional(np.float32(score), unique=False, precision=4)\n",
    "                                global_score_print = np.format_float_positional(np.float32(global_score), unique=False, precision=4)\n",
    "                                seq_rec_print = np.format_float_positional(np.float32(seq_recovery_rate.detach().cpu().numpy()), unique=False, precision=4)\n",
    "                                sample_number = j*BATCH_COPIES+b_ix+1\n",
    "                                f.write('>T={}, sample={}, score={}, global_score={}, seq_recovery={}\\n{}\\n'.format(temp,sample_number, score_print, global_score_print, seq_rec_print, seq)) #write generated sequence\n",
    "                if save_score:\n",
    "                    np.savez(score_file, score=np.array(score_list, np.float32), global_score=np.array(global_score_list, np.float32))\n",
    "                if save_probs:\n",
    "                    all_probs_concat = np.concatenate(all_probs_list)\n",
    "                    all_log_probs_concat = np.concatenate(all_log_probs_list)\n",
    "                    S_sample_concat = np.concatenate(S_sample_list)\n",
    "                    np.savez(probs_file, probs=np.array(all_probs_concat, np.float32), log_probs=np.array(all_log_probs_concat, np.float32), S=np.array(S_sample_concat, np.int32), mask=mask_for_loss.cpu().data.numpy(), chain_order=chain_list_list)\n",
    "                \n",
    "                all_probs_list_list.append(all_probs_list)\n",
    "                all_sample_list_list.append(S_sample_list)\n",
    "                chain_M_pos_list.append(chain_M_pos)                \n",
    "    t1 = time.time()\n",
    "    dt = round(float(t1-start_time), 4)\n",
    "    num_seqs = len(temperatures)*NUM_BATCHES*BATCH_COPIES\n",
    "    total_length = X.shape[1]\n",
    "    print(f'{num_seqs} sequence of length {total_length} generated in {dt} seconds with score of {score_print} and sequence recovery of {seq_rec_print}')\n",
    "    \n",
    "    \n",
    "    return all_probs_list_list, all_sample_list_list,chain_M_pos_list,score_print, seq_rec_print\n",
    "\n",
    "printmd(\"MPNN Function defined\", color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e384e77",
   "metadata": {},
   "source": [
    "# Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e42b3457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'>Check that your protein states are in the same order as your weights!</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>1 => 1_binary_noMSA <= 1.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>2 => 2_6bp_noMSA <= 1.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>3 => 3_8bp_noMSA <= 1.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Dictionary of states and associated weights\n",
    "# match input PDB name and number between -1 and 1\n",
    "# if state is omitted it gets assigned 1 by default\n",
    "state_weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "### TODO: sort the JSON entries based on PDB name\n",
    "\n",
    "### Check\n",
    "printmd(\"Check that your protein states are in the same order as your weights!\", color=\"red\")\n",
    "i = 0\n",
    "for ix, protein in enumerate(dataset_valid):\n",
    "    j = i+1\n",
    "    printmd(str(j)+' => '+protein['name']+' <= '+str(state_weights[i]), color=\"blue\")\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1713b5c9-f8a4-46af-9911-b7629b9858e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f9e5acd91742e3b2bbdc92a14bc6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating designs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>Seed value for design NeoCas_Trial3_1 is: 5324</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating probabilities for: 1_binary_noMSA\n",
      "Generating probabilities for: 2_6bp_noMSA\n",
      "Generating probabilities for: 3_8bp_noMSA\n",
      "Generating probabilities for: 4_10bp_noMSA\n",
      "Generating probabilities for: 5_16bp_noMSA\n",
      "Generating probabilities for: 6_18bpchck_noMSA\n",
      "Generating probabilities for: 7_18bpcat_noMSA\n",
      "1 sequence of length 1368 generated in 148.52 seconds with score of 1.8485 and sequence recovery of 0.3251\n",
      "----------------------------------------\n",
      "Designed sequence:\n",
      "APEPWSIGLDIGTDSVGYAVIDENFEVPKKTFPVSGNTDVKEREKNLIGVLLFPPAKSREARRRRRRRRRERRRRQNRLELLEEIFAPELAKEDPNYLARLRERHLPPEDRKLSRHPLFGNEEKEKAYKEKYPTIEALILDLVESTEKQPLRLIYLALRYLIRNRGNFRIKGELDPSNNDIQALFRELVDTYNALFPENPIDVEGVDFESILTSDLSPEERLDELIAAIPGVTKDSFFGNLLALALGLTPNFSPNFGLDEPALLDLDDPDYEERLAKLLSEVGEEYKPLFDAARKLGDAILLSRILKVDPSTTKSPLAARKIEIYERHHEDLEKLKELIKKQAPELYDEIFEDKSGNGYAGYEDGTATYEEFYAYIRPILESLPGTEELLELLEAGTLLRKLRDPRNKAIPRDLRLHELSAILDNQEPYYPFLKENKEEILTILTFRVPEYVGPLSRGNSPDSHAVFKKNEPVTPWNFEEIVDFVASARNYVRRKTPRDPLLPGKPVLPKNSLTYQEFLVYNELNGIKYLTEGDKKPRYLTGEEKEDIIEKLFLKNETVTVEQIKEEFFKKVKKYKNVEISGVGGTFKATLSTYHTLLEILKDEEFLLDPENQPIIEEIIEILTTFKDPELKRKALKKYKHLFSEEQMEKLVLLRFSGWGELSLELITGIKDEKTGKSILDYLKDDGEENLTFEEILNDPSLSFKKQIEEARKGGNGLSLEERIASLPASPAMKKGILLALEILDELVELLGNQKPELIAIEIEPSLKSDDEKEARAKERLERIKKGLAKLGSDLLEKYPVTAEELLDLKLYLYYRQNGRDFYTDEELDLDKLDEYEIDHIIPLSYKDDDSEDNLVLVKSSSNKGKGDDVPSKEVVEKMKEYWQRLLDAGLISEETYNNLTRALTGGLTKEDKADLIKSQLVVEDPFVKELAELIDELLNTEKDENGKLIIKTKVVLLKGSLVDRFREEFGFYEVPEINPRHHAYDAYLTAVLGNKLLKRYPELKSLFVYGDFPYVDLKELIAPGGSEDGLASFKLFYYANILNFLDDTVKLRNGEVVKNPEVVYNPKTGEVVFDRSTDLKIIREVLNLPDINIVVEEEVRTGPDADPEWLPPEDSDRLIPRAPDWDPKLYGGYRNPNVAYSVLVRAEKPVGPEKVLVPVLELVGITIRDKAAFEKDPIAYLESLGYKNVDEEKIIELPRNTLFLLENGRRLLLASATELWNGNVLALPLELINWLYLASNLNGLQGSPEELAAKRQYVEENRHLLDEIVDLIEDFAKRYVKDPEALAKLLAAWEKNKDLPLTELARNLINAFTLVRLGEPEAFKFGDTRIPRVRRTDTSELLDGILIHRDITGLNRTLIDLSLLGKE\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>Seed value for design NeoCas_Trial3_2 is: 5006</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating probabilities for: 1_binary_noMSA\n",
      "Generating probabilities for: 2_6bp_noMSA\n",
      "Generating probabilities for: 3_8bp_noMSA\n",
      "Generating probabilities for: 4_10bp_noMSA\n",
      "Generating probabilities for: 5_16bp_noMSA\n",
      "Generating probabilities for: 6_18bpchck_noMSA\n",
      "Generating probabilities for: 7_18bpcat_noMSA\n",
      "1 sequence of length 1368 generated in 145.6462 seconds with score of 1.8014 and sequence recovery of 0.3199\n",
      "----------------------------------------\n",
      "Designed sequence:\n",
      "APEPWSIGLDIGTDSVGFAVIDENFRVPTKTFPVSGNTDVKSRKKNLIGVLLFPPAKSDEEERRRRRARRRRRRRANRLELLEEIFAPELAKEDPNYLARLEERHLPPEDRKYSRHPLFGNEEKEEAFKKKFPTIEALILDLVESKEKQDLRDIYEALRYLIRNRGNFKIEGELDPSKTDVQALFRELVDTYNALFPENPIDTTGVDFEAILTSDKSKEERLDKLIAAIPGVTEDGFFGKLLALALGLTPNFSPSFNLPEPALLDLRDPDYEERLAELLSEIGPEYKPLFDAAKKLSDAIYLSRILKVDPSTTKAPLAARKIEILKQHHEQLEKLKELIKAQAPELYDEIFEDTSGNGYAGYEDGTATYEEFYAYIRPILESLEGTEELLEALDKGTLLRKIRDEDNKAIPRDLRLGTLSAILDNQQPYYPFLAENKEEILNILTFRVPEYIGPLSRGNSPDSHAVFKTNEPVTPWNFEEIVDYVASAERYVERKTPRDPLLPDEPVLPKNSLTMQEFLVYNELNKVKYLTPGLKEPRYLTGEEKLAIYELLFLKKEVVTVEQLKEEYFKKVKKYKNVEISGIKGTFKATLSTYHTLLKILKDEEFLLDPSNRPIIEEIIEILTTFKDLELREKELSKYKHLFSEEQMEKLVLLRYSGWGTLSRELITGLKDKKTGKSILDFLKDDGEENKTFEEILNDPSLSFGKQIEEARKGGNGLSLEERIASLKASPALKKGILLALEILEELVKLLGNQKPELIVIEIEPEPKSDKEKEERAKERLARIKKGHAKLGSDLLEEYPVTPEELLDEKLYLYYRQNGKDFYTDEELDLEELDEYEIDHIIPLSYLDDDSLDNLVLVKDSSNKGKNDDVPSKEVVEKMEEYWQRLLDAGLISEETYDNLTRALTGGLTKEDKADLIKSQLVVEDPIARELARLIDRLLNTEKDENGKLIRKTKVVRLNGSLVDEFREEFGFYVVPEVNEKHHAYDAYLTAVIGNKLLKLYPELKALFVYGDFPKEDLESLIAPSGDELGKASFKLFYFANILNFLLDTIELRNGTIIKNPEVVKNPKTGEVVFDRKTDLKIIREVLNLPDINIVVKEEVRTGPPADPEWLPPEDSPRLIPRRPDWDPRLYGGYRNPNVAYSVLVLAEVPVGPEKVLVPVKELVGITIRDRAAFEADPIAYLESKGYEDVDESKLIELPRYTLFLLENGRRLLLKSATELWNGNELALPLELINWLYLASNLNGLQGSKEELAKLEAYVEENKSLLDEIVDLIEDFAKKYVRDPEQLAKLLAAWEKNKDLPLTELARALINAFTLVRLGEPEAFKFGDTRIPRARRTDTDELLDGILIHRSITGLKQTLIDLSLLGKE\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>Seed value for design NeoCas_Trial3_3 is: 7378</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating probabilities for: 1_binary_noMSA\n",
      "Generating probabilities for: 2_6bp_noMSA\n",
      "Generating probabilities for: 3_8bp_noMSA\n",
      "Generating probabilities for: 4_10bp_noMSA\n",
      "Generating probabilities for: 5_16bp_noMSA\n",
      "Generating probabilities for: 6_18bpchck_noMSA\n",
      "Generating probabilities for: 7_18bpcat_noMSA\n"
     ]
    }
   ],
   "source": [
    "# Generate probabilities\n",
    "final_sequences = []\n",
    "\n",
    "for i in tqdm(range(1, num_of_sequences+1, 1), desc=\"Generating designs\"):\n",
    "    # set seed\n",
    "    if seed_set is None:\n",
    "        seed = int(np.random.randint(0, high=9999, size=1, dtype=int)[0])\n",
    "    else:\n",
    "        seed = seed_set\n",
    "\n",
    "    sequence_name = design_name+'_'+str(i)\n",
    "    printmd(\"Seed value for design \"+sequence_name+\" is: \"+str(seed), color=\"blue\")\n",
    "    all_probs_list_list, all_sample_list_list,chain_M_pos_list,mpnn_score, sequence_recovery = run_multistate_mpnn(dataset_valid, seed)\n",
    "    mean_pssm_list = []\n",
    "\n",
    "    for j in range(len(all_probs_list_list)):\n",
    "        design = all_probs_list_list[j]\n",
    "        weight = state_weights[j]\n",
    "        mean_design_pssm = design[0].mean(axis=0)\n",
    "        mean_pssm_list.append(mean_design_pssm*weight)\n",
    "        \n",
    "    np.array(mean_pssm_list).shape\n",
    "    master_pssm = np.array(mean_pssm_list).mean(axis=0)\n",
    "    idx = np.argmax(master_pssm,axis=1)\n",
    "    result= (all_sample_list_list[0]*(1-chain_M_pos_list[0].numpy()))[0][0]+idx*(chain_M_pos_list[0][0].numpy())\n",
    "\n",
    "    # Sample most probable sequence\n",
    "    print(40*'-')\n",
    "    l = []\n",
    "    for res in result:\n",
    "        l.append(alphabet[int(res)])\n",
    "    master_seq = ''.join(l)\n",
    "#     master_seq = max_sequence(master_pssm)\n",
    "    print(\"Designed sequence:\")\n",
    "    print(master_seq)\n",
    "\n",
    "    final_seq = [sequence_name, master_seq, str(mpnn_score), str(sequence_recovery), str(seed), str(sampling_temp), str(backbone_noise), \" \".join([str(w) for w in state_weights]), positions_to_fix]\n",
    "    final_sequences.append(final_seq)\n",
    "    \n",
    "# generate dataframe\n",
    "final_seq_list = pd.DataFrame(final_sequences, columns=['Design Name', 'Sequence', 'MPNN Score', 'Sequence Recovery', 'Seed', 'Temperature', 'Backbone noise', 'State Weights', 'Fixed Positions'])\n",
    "final_seq_list.to_csv(folder_for_outputs+design_name+\"_designs.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display Designs\n",
    "design_df = pd.read_csv(folder_for_outputs+design_name+\"_designs.csv\", names=['Design Name', 'Sequence', 'MPNN Score', 'Sequence Recovery', 'Seed', 'Temperature', 'Backbone noise', 'State Weights', 'Fixed Positions'])\n",
    "design_df.sort_values(['MPNN Score', 'Sequence Recovery'], inplace=True, ascending=[True, False])\n",
    "design_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa293287",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save FASTA\n",
    "save_sequences = []\n",
    "\n",
    "for index, row in tqdm(design_df.iterrows(), desc=\"Saving FASTA\"):\n",
    "    print(\"Saving sequence: \"+row['Design Name'])\n",
    "    save_sequences.append('>%s\\n%s\\n'%(row['Design Name'],row['Sequence']))\n",
    "    \n",
    "with open(folder_for_outputs+design_name+\"_designs.fasta\", 'w+') as fm:\n",
    "    for line in save_sequences:\n",
    "        fm.write(line)\n",
    "fm.close()\n",
    "printmd(\"FASTA output generated\", color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make weblogo of sequences\n",
    "import logomaker\n",
    "def make_logoplot(seqs):\n",
    "    arr = np.asarray([list(s) for s in seqs])\n",
    "    dfdict = {}\n",
    "    for pos in range(arr.shape[1]):\n",
    "        dfdict[pos] = {n:v/arr.shape[0] for n,v in zip(*np.unique(arr[:,pos],return_counts=True))}\n",
    "    df = pd.DataFrame.from_dict(dfdict,orient='index')\n",
    "    df.fillna(0,inplace=True) \n",
    "    logo = logomaker.Logo(df,color_scheme='weblogo_protein', width=0.95,figsize=(600,3))\n",
    "    logo.style_xticks(anchor=0, spacing=5)\n",
    "    logo.ax.set_ylabel('Occurence')\n",
    "    logo.ax.set_xlabel('Position')\n",
    "    logo.fig.tight_layout()\n",
    "    plt.savefig(folder_for_outputs+design_name+'_weblogo.png', dpi=100, bbox_inches=\"tight\")\n",
    "\n",
    "logo_seq = []\n",
    "for index, row in design_df.iterrows():\n",
    "    logo_seq.append(row['Sequence'])    \n",
    "\n",
    "make_logoplot(logo_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca4de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
